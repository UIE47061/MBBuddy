# backend/main.py
from contextlib import asynccontextmanager
from fastapi import FastAPI
from api import ai_api
from fastapi.middleware.cors import CORSMiddleware
from api import participants_api
from api import network_api
from api import mindmap_api
from api import hostStyle_api

# è¨­ç½®æ—¥èªŒ
from utility.logger import setup_logger
logger = setup_logger("mbbuddy")

@asynccontextmanager
async def lifespan(app: FastAPI):
    """æ‡‰ç”¨ç”Ÿå‘½é€±æœŸç®¡ç†"""
    # Startup
    logger.info("ğŸš€ MBBuddy å¾Œç«¯æœå‹™å•Ÿå‹•ä¸­...")
    
    # é è¼‰å…¥ CPU LLM æ¨¡å‹
    try:
        from api.local_llm_client import local_llm_client
        logger.info("ğŸ“¥ é–‹å§‹é è¼‰å…¥ CPU LLM æ¨¡å‹...")
        
        # æª¢æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        model_dir = local_llm_client.models_dir / "qwen2-1.5b"
        if model_dir.exists():
            model_files = list(model_dir.glob("*.gguf"))
            if model_files:
                model_path = str(model_files[0])
                logger.info(f"ğŸ“ æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {model_path}")
                
                # é è¼‰å…¥æ¨¡å‹
                success = await local_llm_client.load_model(model_path, "qwen2-1.5b")
                if success:
                    logger.info("âœ… CPU LLM æ¨¡å‹é è¼‰å…¥æˆåŠŸ")
                else:
                    logger.warning("âš ï¸ CPU LLM æ¨¡å‹é è¼‰å…¥å¤±æ•—")
            else:
                logger.warning("âš ï¸ æ‰¾ä¸åˆ° .gguf æ¨¡å‹æ–‡ä»¶")
        else:
            logger.warning("âš ï¸ æ¨¡å‹ç›®éŒ„ä¸å­˜åœ¨ï¼Œå°‡åœ¨é¦–æ¬¡èª¿ç”¨æ™‚ä¸‹è¼‰")
            
    except Exception as e:
        logger.error(f"âŒ é è¼‰å…¥ CPU LLM æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    # æ¸¬è©¦ AnythingLLM é€£æ¥
    try:
        from api.ai_client import ai_client
        from api.ai_config import ai_config
        
        logger.info("ğŸ”— æ¸¬è©¦ AnythingLLM é€£æ¥...")
        logger.info(f"   ç›®æ¨™: {ai_config.base_url}")
        logger.info(f"   å·¥ä½œå€: {ai_config.workspace_slug}")
        
        # ç°¡å–®çš„é€£æ¥æ¸¬è©¦
        test_result = await ai_client.test_connection()
        
        # æ­£ç¢ºæª¢æŸ¥é€£æ¥ç‹€æ…‹
        if test_result.get("status") == "success":
            workspace_count = len(test_result.get("workspaces", []))
            logger.info(f"âœ… AnythingLLM é€£æ¥æ­£å¸¸ (æ‰¾åˆ° {workspace_count} å€‹å·¥ä½œå€)")
        else:
            logger.error("âŒ AnythingLLM é€£æ¥å¤±æ•—")
            logger.error(f"   éŒ¯èª¤è¨Šæ¯: {test_result.get('message', 'æœªçŸ¥éŒ¯èª¤')}")
            if "response" in test_result:
                logger.error(f"   å›æ‡‰å…§å®¹: {test_result['response']}")
            logger.error("   è«‹æª¢æŸ¥:")
            logger.error("   1. AnythingLLM æœå‹™æ˜¯å¦æ­£åœ¨é‹è¡Œ")
            logger.error("   2. ANYTHINGLLM_API_KEY æ˜¯å¦æœ‰æ•ˆ")
            
    except Exception as e:
        logger.error(f"âŒ AnythingLLM é€£æ¥æ¸¬è©¦æ™‚ç™¼ç”Ÿç•°å¸¸: {e}")
        import traceback
        logger.error(f"   è©³ç´°å †ç–Š:\n{traceback.format_exc()}")
    
    logger.info("ğŸ‰ MBBuddy å¾Œç«¯æœå‹™å•Ÿå‹•å®Œæˆï¼")
    
    yield
    
    # Shutdown
    logger.info("ğŸ›‘ MBBuddy å¾Œç«¯æœå‹™æ­£åœ¨é—œé–‰...")
    
    try:
        from api.local_llm_client import local_llm_client
        if local_llm_client.is_model_loaded():
            local_llm_client.unload_model()
            logger.info("âœ… CPU LLM æ¨¡å‹å·²å¸è¼‰")
    except Exception as e:
        logger.error(f"âŒ å¸è¼‰æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    logger.info("ğŸ‘‹ MBBuddy å¾Œç«¯æœå‹™å·²é—œé–‰")

app = FastAPI(title="MBBuddy API", lifespan=lifespan)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(participants_api.router)
app.include_router(ai_api.router)
app.include_router(network_api.router)
app.include_router(mindmap_api.router)
app.include_router(hostStyle_api.router)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8001, reload=True)
